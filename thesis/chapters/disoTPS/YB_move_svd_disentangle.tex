Alternatively, the constrained optimization problem \eqref{eq:disoTPS_YB_move_standard} can be solved via two successive SVDs with an optional disentangling prodcedure with the goal of reducing the truncation error or some entanglement measure. This is is a similar algorithm to the one used for the MM in isoTPS \cite{cite:efficient_simulation_of_dynamics_in_two_dimensional_quantum_spin_systems}, compare section \ref{sec:tensors_and_tensor_networks_isometric_tensor_product_states_in_2D}. The algorithm is sketched in figure \figref{fig:yb_move_svd_disent} and is made up of three main steps.
\begin{enumerate}
	\item We start by contracting the tensors $T$, $W_1$ and $W_2$ into a single tensor $\Psi$. This tensor is then split from left to right via a truncated SVD
	\begin{equation}
		\Psi = ASV^\dagger = A\left(SV^\dagger\right) \eqqcolon A\theta
	\end{equation}
	as shown in figure \figref{fig:yb_move_svd_disent_a}. The bond dimension is truncated to $D^2$.
	\item Next, we split the index of the bond connecting $A$ and $\theta$ into two indices of dimension $D$ each, see figure \figref{fig:yb_move_svd_disent_b}. To proceed, we note that there exists a degree of freedom on the bonds connecting $A$ and $\theta$: A unitary $U$ and its adjoint can be inserted as shown in the second step of figure \figref{fig:yb_move_svd_disent_b} without changing the result of the contraction
	\begin{equation}
		AU^\dagger U\theta = \left(AU^\dagger\right)\left(U\theta\right) \eqqcolon T^\prime \tilde{\theta}.
	\end{equation}
	The unitary $U$ can be chosen to minimize the truncation error of the next step by \textit{disentangling} the tensor $\theta$. We will discuss procedures of finding such a \textit{disentangling unitary} on the next page.
	\item In the last step, the tensor $\tilde{\theta}$ is split vertically into $W_1^\prime$ and $W_2^\prime$ using a truncated SVD as shown in figure \figref{fig:yb_move_svd_disent_c}. Here, the bond dimension is truncated to $\chi$. We end up with the three tensors $T^\prime$, $W_1^\prime$ and $W_2^\prime$, completing the YB move.
\end{enumerate}
Before we discuss the disentangling procedure, two comments about step two of the above algorithm are in order. First, there exists a degree of freedom for splitting the bond index, because applying the same permutations to the columns of $A$ and rows of $\theta$ does not change the result of contracting the network. However, this degree of freedom is fixed by the disentangling process, making the exact permutation of the bond splitting irrelevant. Second, note that near the edges of the lattice it can happen that the matrizized tensor $\Psi$ has $\tilde{\chi} < D^2$ rows. In this case, the bond dimension after the SVD will also be $\tilde{\chi}$ and we cannot simply split the bond into two bonds of dimension $\chi_1=\chi_2=D$. Instead, we choose a splitting $\chi_1 \le D$, $\chi_2 \le D$ such that $\chi_1\cdot\chi_2$ is maximized, while it must still hold $\chi_1\cdot\chi_2\le\tilde{\chi}$. We additionally prefer "equal" splittings $\chi_1\approx\chi_2\approx\sqrt{\tilde{\chi}}$ if possible. One can find such a splitting easily by computing all possible combinations of $\chi_1$ and $\chi_2$ and keeping only the best one, which has a computational cost of $\mathcal{O}\left(\sqrt{\tilde{\chi}}\right) = \mathcal{O}\left(D\right)$. \par
\begin{figure}
	\centering
	\subcaptionbox{\label{fig:yb_move_svd_disent_a}}
	{%
		\includegraphics[scale=1.0]{figures/tikz/disoTPS/yang_baxter_move_svd/yang_baxter_move_svd_a.pdf}
	}
	\subcaptionbox{\label{fig:yb_move_svd_disent_b}}
	{%
		\includegraphics[scale=1.0]{figures/tikz/disoTPS/yang_baxter_move_svd/yang_baxter_move_svd_b.pdf}
	}
	\subcaptionbox{\label{fig:yb_move_svd_disent_c}}
	{%
		\includegraphics[scale=1.0]{figures/tikz/disoTPS/yang_baxter_move_svd/yang_baxter_move_svd_c.pdf}
	}
	\caption{The YB move is made up of three steps as explained in the text.\todo{Change X to A}}
	\label{fig:yb_move_svd_disent}
\end{figure}
\subsubsection*{The disentangling process}
We will now discuss the problem of finding a good disentangling unitary $U$ for step 2 of the above algorithm, which is crucial for the performance of the YB move. The problem can be formulated as follows: Given the tensor $\theta$ that is obtained after splitting the index in step two, find a unitary $U$ minimizing a cost function $f(U, \theta)$. In the following, let $\tilde{\theta}_{(l,i),(j,r)}$ be the $\chi D^2\times \chi D^2$ matrix that is obtained by reshaping the contraction $\tilde{\theta}_{l,i,j,r} = \sum_{i^\prime,j^\prime} U_{i,j,i^\prime,j^\prime}\theta_{l,i,j,r}$ into a matrix as shown in figure \figref{fig:disentangling_theta_definition}. Let further $\tilde{\theta} = XSY$ denote the SVD of $\tilde{\theta}$. We discuss two cost functions. The first cost function is simply given by the truncation error
\begin{equation}
	\label{eq:YB_move_disent_cost_function_truncation_error}
	f_\text{trunc}\left(U,\theta\right) = \sum_{j = \chi+1}^{\chi D^2}S_j^2
\end{equation}
arising in step three of the YB move. Alternatively, one can think of $|\tilde{\theta}\rangle \coloneqq \sum_{(l,i), (j,r)}\tilde{\theta}_{(l,i),(j,r)}|(l,i), (j, r)\rangle$ as a bipartite state in an orthogonal basis and consider as a cost function the Rényi-entropy
\begin{equation}
	\label{eq:renyi_entropy}
	f_\text{Rényi}\left(U,\theta,\alpha\right) = \frac{1}{1-\alpha}\log\Tr\left(\rho^\alpha\right) = \frac{1}{1-\alpha}\log\left(\sum_{j=1}^{\chi D^2}s_i^{2\alpha}\right),
\end{equation}
where $\alpha\in[0,\infty)$ and $\rho = \Tr_{(j, r)}(|\tilde{\theta}\rangle\langle\tilde{\theta}|)$ is the reduced density matrix obtained by tracing out one of the subsystems, see figure \figref{fig:disentangling_rho_definition}. In the last step of \eqref{eq:renyi_entropy} we used the fact that the eigenvalues of $\rho$ are the squares of the singular values of $\tilde{\theta}$. The Rényi-entropy can be used as a measure of entanglement. It approaches the Von-Neumann entanglement entropy for $\alpha\rightarrow 1$. It can be shown that the truncation error is bounded by the Rényi-entropy if $\alpha < 1$ \cite{cite:mps_represent_ground_states_faithfully}, which is a motivation for using $f_\text{Rényi}$ as a cost function. For $\alpha > 1$ such a bond cannot generally be given. However, optimizations of Rényi-entropies with $\alpha > 1$ are often simpler to perform and still achieve good results in practice \cite{cite:isometric_tensor_network_states_in_two_dimensions, cite:efficient_simulation_of_dynamics_in_two_dimensional_quantum_spin_systems, cite:finding_purifications_with_minimal_entanglement}. Setting $\alpha = 2$ yields the Rényi-entropy
\begin{equation}
	\label{eq:renyi_entropy_alpha_2}
	f_\text{Rényi}\left(U,\theta,\alpha=2\right) = -\log\Tr\rho^2,
\end{equation}
which can easily computed by contracting the tensor network shown in figure \figref{fig:disentangling_evenbly_vidal_algorithm}(a) without needing to perform an SVD. The cost function $f_\text{Rényi}\left(U,\theta,\alpha=2\right)$ can be minimized using the Evenbly-Vidal algorithm as proposed in \cite{cite:finding_purifications_with_minimal_entanglement}. First, the minimization problem can be rewritten as a maximization problem
\begin{equation}
	U^\text{opt} = \underset{U^\dagger U = \id}{\argmin} f_\text{Rényi}\left(U,\theta,\alpha=2\right) = \underset{U^\dagger U = \id}{\argmax}\Tr\rho^2.
\end{equation}
We proceed by taking one tensor $U$ out of the network $\Tr\rho^2$ and contracting all other tensors into the environment $E$ as shown in figure \ref{fig:disentangling_evenbly_vidal_algorithm}(b). We now treat $E$ as if it were independent of $U$ and update $U\leftarrow AB^\dagger$, where $A$ and $B$ are obtained by taking the SVD $E=A\Lambda B^\dagger$. This is repeated until convergence. For details on the Evenbly-Vidal algorithm see appendix \ref{sec:evenbly_vidal_algorithm}. In practice it is observed that this algorithm for minimizing $f_\text{Rényi}\left(U,\theta,\alpha=2\right)$ converges very quickly \cite{cite:efficient_simulation_of_dynamics_in_two_dimensional_quantum_spin_systems}.\par
\begin{figure}
	\centering
	\includegraphics[scale=1]{figures/tikz/disoTPS/theta_tilde_contraction/theta_tilde_contraction.pdf}
	\caption{The disentangling unitary $U$ is contracted with the wave function tensor $\theta$ to form $\tilde{\theta}$, which is subsequently split via an SVD $\tilde{\theta} = XSY^\dagger$.}
	\label{fig:disentangling_theta_definition}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{figures/disoTPS/disentangling_rho_definition.jpeg}
	\caption{Definition of the reduced density matrix $\rho$.}
	\label{fig:disentangling_rho_definition}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{figures/disoTPS/disentangling_evenbly_vidal_algorithm.jpeg}
	\caption{(a) Tensor network for the computation of the cost function \eqref{}. (b) Taking out one unitary $U$, the tensor network is contracted into the environment $E$.}
	\label{fig:disentangling_evenbly_vidal_algorithm}
\end{figure}
Minimizing the truncation error $f_\text{trunc}\left(U,\theta\right)$ and general Rényi-entropies $f_\text{Rényi}\left(U,\theta,\alpha\neq2\right)$ is a harder problem. We follow the approach of \cite{cite:isometric_tensor_network_states_in_two_dimensions, cite:efficient_simulation_of_dynamics_in_two_dimensional_quantum_spin_systems} and use Riemannian optimization \cite{cite:optimization_on_matrix_manifolds, cite:riemannian_optimization_isometric_tensor_networks, cite:riemannian_geometry_automatic_differentiation_quantum_physics, cite:pymanopt} to solve the optimization problem. The idea of Riemannian optimization is to generalize common optimization algorithms defined in Euclidian vector spaces, such as Gradient Descent or Conjugate Gradients, to Riemannian manifolds. The set of all isometric matrices of shape $n\times m$ is a Riemannian manifold called the Stiefel manifold $\Stiefel$. A special case is the set of all unitary matrices of shape $n\times n$, $U(n)=\text{St}(n, n)$, over which we want to optimize here. Riemannian optimization over the Stiefel manifold is discussed in more detail in appendix \ref{app:riemannian_optimization_of_isometries}. \par
A typical optimization algorithm iteratively improves an iterate $U_k\in\Stiefel$, $k=1,2,\dots$ until a local minimum of the cost function $f$ is found. In Riemannian optimization, the gradient of the cost function $\nabla f\left(U_k\right)$ is restricted to the tangent space $T_{U_k}\Stiefel$ of the iterate $U_k$, which we visualize in figure \figref{fig:disentangling_riemannian_optimization}. The gradient can be computed either analytically or via automatic differentiation \cite{cite:riemannian_geometry_automatic_differentiation_quantum_physics, cite:pymanopt}. Optimization algorithms typically compute a search direction $\xi\in T_{U_k}\Stiefel$ and a step size $\alpha \in \mathbb{R}$ from the gradient. In an optimization algorithm defined on an Euclidean vector space one would then move along this direction as
\begin{equation}
	\tilde{U}_{k+1} = U_k + \alpha\xi.
\end{equation}
However, $\tilde{U}_{k+1}$ is in general not an element of the manifold. To ensure $U_{k+1}\in\Stiefel$, one can introduce a \textit{retraction} $R_\xi:\mathbb{R}\to\Stiefel$. One can think of $R_\xi\left(\alpha\right)$ as moving along the direction of $\xi$ while staying on the manifold. As $\alpha$ increases, we move further along the path defined by the retraction, with $R_\xi(0) = U_k$. Different retractions can be chosen, varying by how well they perform in optimization problems and by how hard they are to compute. Here we choose the retraction
\begin{equation}
	R_\xi(\alpha) = \qf\left(U_k + \alpha\xi\right),
\end{equation}
where $\qf\left(A\right)$ is the Q-factor of the QR-decomposition $A = QR$. This retraction is particularly easy to compute and yields good results in practice. \par
Many optimization algorithms such as Conjugate Gradients require gradients from previous iterates for computing a search direction at the current iterate. In Riemannian optimization, these gradients must first be brought from the tangent spaces of previous iterates to the tangent space of the current iterate. This is handled by a so-called \textit{vector transport} $T_{k\rightarrow k+1}\left(\xi_k\right)$, see figure \figref{fig:disentangling_riemannian_optimization}. \par
Finally, for optimization algorithms of second order such as the trust-region method, one needs to generalize the notion of the hessian-vector product to Riemannian manifolds. This generalization is given by the \textit{Riemannian connection} \cite{cite:optimization_on_matrix_manifolds}. For the Stiefel manifold this is simply given by projecting the hessian vector product of the embedding euclidean vector space $\mathbb{C}^{n\times p}$ to the tangent space. \par
We used two algorithms for solving the disentangling optimization problem, Conjugate Gradients (CG) and the Trust-Region Method (TRM). CG uses the accumulated gradients of previous iterations to compute an improved search direction, trying to achieve superlinear convergence. CG is discussed in more details in appendix \ref{}\todo{add ref}. The TRM approximates the cost function around the current iterate through a quadratic function using the hessian vector product. This approximate cost function is then minimized within a region of radius $\Delta\in\mathbb{R}$ using truncated Conjugate Gradients (tCG), which converges quickly for the quadratic approximation. Depending on the quality of the approximation at the current iterate one can then shrink or enlarge the trust region. TRM is able to achieve superlinear convergence on many cost functions \cite{cite:optimization_on_matrix_manifolds}. For more details, see appendix \ref{}\todo{add ref}. \par
The gradients and hessian vector products of the cost functions \eqref{eq:YB_move_disent_cost_function_truncation_error} and \eqref{eq:renyi_entropy} can be computed analytically with a computational cost of $\mathcal{O}()$\todo{Complexity berechnen}, see appendix \ref{app:computation_of_gradient_and_hvp_for_riemannian_optimization} for a derivation.
\subsubsection*{Approximate gradients and hessian vector products}
\begin{figure}
	centering
	\includegraphics[width=0.8\textwidth]{figures/disoTPS/approximate_qr_decomposition.jpeg}
	\caption{\todo{write caption}}
	\label{fig:approximate_qr_decomposition}
\end{figure}
The cost of both CG and the TRM are dominated by the computation of the gradient and hessian vector product, particularly by the SVD $\tilde{\theta} = XSY$ and contractions involving $X$ and $Y$. The reason for this is the large bond dimension $\chi D^2$ of the bond connecting $X$ and $Y$. We thus propose to approximate the gradient and hessian vector product by only performing an approximate SVD $\tilde{\theta} \approx \tilde{X}\tilde{S}\tilde{Y}$. The algorithm for performing this approximate SVD is inspired by \cite{cite:fast_time_evolution_of_mps_using_qr}, where a similar algorithm was used to speed up TEBD updates for MPS. We sketch the algorithm in figure \figref{fig:approximate_qr_decomposition}. First, an approximate QR-decomposition $\tilde{\theta} = QR$ is performed by variationally minimizing the distance $\lVert \tilde{\theta} - QR \rVert$, alternatingly optimizing the tensors $Q$ and $R$ as shown in figures \figref{fig:approximate_qr_decomposition}(b) and \figref{fig:approximate_qr_decomposition}(c) respectively. A standard SVD is then performed on the $R$-factor of the approximate QR-decomposition as $R = A\tilde{S}\tilde{Y}$, and the contraction $\tilde{X} = QA$ finalizes the decomposition. It is observed that the variational minimization converges very quickly in practice, especially if a good initialization is chosen. As the iterates $U_k$ are expected to only change slightly each iteration of CG or TRM, one can simply use the result $Q, R$ obtained in the approximate QR-decomposition of the previous iteration as initialization for the current iteration. We observe that the variational optimization in practice converges in less than $5$ iterations.
\subsubsection*{Comparison of different disentangling algorithms}
\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{figures/disoTPS/disentangling_riemannian_optimization.jpeg}
	\caption{In this figure, a visualization of optimization on Riemannian manifolds is given. The iterate $U_k$ (left) is updated along the search direction $\xi_k$, which is an element of the tangent space $T_{U_k}\Stiefel$. The next iterate $U_{k+1}$ is computed with the retraction $R_\xi\left(\alpha\right)$, where $\alpha\in\mathbb{R}$ is the step size. For the computation of the next search direction $\xi_{k+1}$ the previous search direction $\xi_k$ is needed, which is brought to the tangent space $T_{U_{k+1}}\Stiefel$ of the new iterate $U_{k+1}$ via the vector transport $T_{k\rightarrow k+1}\left(\xi_k\right)$.}
	\label{fig:disentangling_riemannian_optimization}
\end{figure}
We will now compare the different algorithms for solving the disentangling problem.
\input{figures/plots/disoTPS/yb_move_disentangling_renyi_2.tex}
\input{figures/plots/disoTPS/yb_move_disentangling_renyi_0.5_trunc_error.tex}
For this comparison we select a YB move environment $\left\{W_1, W_2, T\right\}$ that was encountered during a ground state search of the transverse field Ising model using imaginary TEBD on disoTPS, see chapter \ref{sec:TFI} for details. The results on other YB environments agree qualitatively with the results we present here. The bond dimensions chosen for the disoTPS are $D = 4$, $\chi = 24$. \par
We first test the Evenbly-Vidal algorithm optimizing the Rényi-entropy with $\alpha = 2$, see figure \figref{fig:disoTPS_disentangling_evenbly_vidal_renyi_2}. The algorithm converges very quickly after only $\approx 20$ iterations. The convergence speed depends drastically on the initialization of the disentangling unitary. We find that a initialization based on an SVD of $\theta$ works best, see appendix \ref{app:initialization_of_disentangling_unitary}. \par
Next we look at the performance of CG and TRM optimizing the Rényi-entropy with $\alpha = 1/2$ and the truncation error in figures \figref{fig:disoTPS_disentangling_cg_trm_renyi_0.5} and \figref{fig:disoTPS_disentangling_cg_trm_truncation_error} respectively. In both cases we observe that the TRM leads to a faster decrease in the cost function for the same number of iterations compared to CG. Remarkably, the approximate versions of both CG and TRM perform not drastically worse than their exact counterparts. Again, the speed of convergence depends strongly on the initialization. Because of the quick convergence of the Evenbly-Vidal algorithm optimizing the $\alpha = 2$ Rényi-entropy we choose its result as an initialization for the optimization algorithms using Riemannian optimization. This achieved the best results in our testing. \par
Lastly we plot the cost function against the walltime of the algorithms in figures \figref{fig:disoTPS_disentangling_cg_trm_renyi_0.5_walltime} and \figref{fig:disoTPS_disentangling_cg_trm_truncation_error_walltime}. For this benchmark, the algorithms were run on a i5-12500 CPU with 6 cores. As one can see, the approximate versions of both CG and TRM run up to an order of magnitude faster while still providing a comparable minimization of the cost function. Thus, in practice, it is often better to choose a larger maximum bond dimension with the approximate disentangling algorithms instead of a smaller maximum bond dimension with the exact algorithms.
