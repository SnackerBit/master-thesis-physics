In general the cost function $f(T)$ is not linear. For example, a non-linear cost function is encountered in the disentangling procedure when optimizing a MERA wave function \cite{}. It was proposed by Evenbly and Vidal \cite{} to linearize the cost function and to update the tensor $T$ iteratively using the closed form solutions from section \ref{sec:orthogonal_procrustes_problem}. Let us assume that the cost function $f(T)$ can be written as the contraction of a tensor network, where in general the tensor $T$ may appear multiple times. We contract all tensors except one of the tensors $T$ into an environment tensor $E_T \in\mathbb{C}^{n\times n}$ and the cost function becomes
\begin{equation}
	f(T)=\Re\Tr\left(E_TT\right).
\end{equation}
We now keep the environment $E_T$ fixed, trating it as if it were independant of $T$, and updating $T$ with the closed form solutions \eqref{eq:orthogonal_procrustes_problem_closed_form_solution} or \eqref{eq:orthogonal_procrustes_problem_simple_case_closed_form_solution}. This is repeated until $T$ is converged. If and how fast $T$ converges depends on the details of the cost fucntion, but connvergence cannot be guaranteed for arbitrary cost functions. This procedure is discussed in more detail and for general cost functions in appendix \ref{app:}\par
One can also use Riemannian optimization for the optimization of general non-linear cost functions of isometries. This method is more powerful but also more involved and is discussed in appendix \ref{app:riemannian_optimization_of_isometries}.