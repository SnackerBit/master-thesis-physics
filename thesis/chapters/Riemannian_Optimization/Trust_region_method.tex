\textit{Trust-region methods} (TRM) are a class of second order optimization techniques that are known for their desirable global convergence properties with a local superlinear rate of convergence \cite{cite:optimization_on_matrix_manifolds, cite:trust_region_methods_on_riemannian_manifolds}. The main idea of the TRM is to locally approximate the cost function $f$ by a quadratic model
\begin{equation}
	m_{X_k}(\eta) = f(X_k) + \langle \nabla f(X_k), \eta\rangle + \frac{1}{2}\langle \text{Hess}f(x_k)[\eta], \eta\rangle \approx f(X_k+\eta),
\end{equation}
in a \textit{trust-region} $\langle\eta,\eta\rangle \le \Delta_k^2$ of radius $\Delta_k$ around the current iterate $X_k$. To define this model, one must compute the Hessian-vector product $\text{Hess}f(X_k)[\eta]$ or an approximation thereof. On Riemannian submanifolds of Euclidean vector spaces, the Hessian can be computed by projecting the directional derivative $\text{D}(\nabla f(X_k))\left[\eta\right]$ of the gradient $\nabla f(X_k)$ in direction $\eta$ from the embedding space to the tangent space of $X_k$ \cite{cite:optimization_on_matrix_manifolds}. Since the Stiefel manifold is such a Riemannian submanifold, it holds
\begin{equation}
	\label{eq:hessian_vector_product}
	\text{Hess}f(X_k)[\eta] = P_{X_k} \text{D}(\nabla f(X_k))\left[\eta\right]
\end{equation}
with the projection \eqref{eq:Stiefel_manifold_project_to_tangent_space}. Optimizing the cost function on the quadratic model inside the trust-region is called the \textit{trust-region subproblem} and can be solved via \textit{truncated Conjugate Gradients} (tCG), which converges quickly on the quadratic model \cite{cite:optimization_on_matrix_manifolds}. The solution of the trust-region subproblem is a vector $\eta_\text{opt}$. The proposed next iterate is then given by moving in the direction $\eta_\text{opt}$ as $X_{k+1} = R_{\eta_\text{opt}}(1)$. To decide if the next iterate is accepted or not, we proceed to compute the quotient
\begin{equation}
	\rho_k \coloneqq \frac{f(X_k) - f(X_{k+1})}{m_{X_k}(0) - m_{X_k}(\eta_\text{opt})}.
\end{equation}
The quotient $\rho_k$ is a measure of how well the cost function is represented by the model $m_{X_k}$. If $\rho_k$ is to small, the model is very inaccurate. In this case the proposed update step must be rejected and the trust-region radius $\Delta_k$ must be reduced. If $\rho_k$ is close to one, the model is a good approximation of the cost function, and thus the update step can be accepted and the trust-region radius increased. If $\rho_k \gg 1$, the model is inaccurate, but the iteration still produces a significant decrease in the cost. One possible strategy in this situation is to accept the update and to increase the trust-region radius, hoping that this behaviour will remain in the following iterations, decreasing the cost further. This finally concludes one iteration of the TRM. For more details on the TRM on Riemannian manifolds, see \cite{cite:optimization_on_matrix_manifolds, cite:trust_region_methods_on_riemannian_manifolds, cite:pymanopt}.